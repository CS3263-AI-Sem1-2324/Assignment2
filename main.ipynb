{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "\n",
    "Welcome to the CS3263 assignment 2, you will complete both the programming part and writing part in this notebook!\n",
    "\n",
    "Group Member 1:\n",
    "* Name:\n",
    "* Student NetID (starts with e, e.g., e1234567):\n",
    "\n",
    "Group Member 2:\n",
    "* Name:\n",
    "* Student NetID (e.g., e1234567):\n",
    "\n",
    "## Programming Part\n",
    "In the programming part, we are going to solve the MDP problem for object rearrangment. This notebook will walk you through to use the environment, to define your own agent, to solve the problems.\n",
    "\n",
    "You are expected to add your codes in the blocks noted by:\n",
    "```python\n",
    "# ------- your code starts here ------- #\n",
    "\n",
    "\n",
    "# ------- your code ends here ------- #\n",
    "```\n",
    "Let's start!\n",
    "\n",
    "## Python Environment Setup\n",
    "To set up the programming environment, three basic packages typer, typing and numpy are needed. You can install them by the way you like.\n",
    "\n",
    "For example, install using pip:\n",
    "\n",
    "```pip install typer numpy```\n",
    "\n",
    "or install using Anaconda:\n",
    "\n",
    "```conda install typer numpy```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from minihouse.robotminihousemodel import MiniHouseV1\n",
    "from minihouse.minihousemodel import state_class, item_object\n",
    "import numpy as np\n",
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minihouse Environment\n",
    "All environments are defined in the minihouse package.\n",
    "\n",
    "Each state and action is represented as an integer. You can use the following functions to convert between the integer and the corresponding state description.\n",
    "\n",
    "```python\n",
    "goal_state = state_class(\n",
    "    robot_agent=None,\n",
    "    human_agent=None,\n",
    "    object_list={\"apple\": item_object(\"apple\", True, \"table\")},\n",
    "    container_list=None,\n",
    "    surface_list=None,\n",
    "    room_list=None,\n",
    ")\n",
    "\n",
    "env = MiniHouseV1(\n",
    "    instruction=\"move the apple to the table\",\n",
    "    goal_state=goal_state,\n",
    ")\n",
    "env.reset()\n",
    "print(env.state)\n",
    "print(env.state_to_index(env.state))\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment: implement policy iteration and value iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are expected to implement the policy iteration and value iteration algorithms to solve the searching problems in the MiniHouse environment. Here, we provide a template for you to implement your own agent. **Remember, copy your implmentation to the `programming.py` file before you submit your assignment. Otherwise, you cannot get any score for coding part.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Here, we introduce the role of each function in this solution template. \n",
    "\n",
    "### Helper functions\n",
    "```python\n",
    "    def get_policy(self, env_nS: int, env_nA: int, env_transition: Callable, \n",
    "                   gamma: float, V: np.ndarray):\n",
    "```\n",
    "This function is used to extract a policy given a value function. The input arguments are:\n",
    "* env_nS: number of states\n",
    "* env_nA: number of actions\n",
    "* env_transition: transition function, which returns a list of tuples (probability, next_state_index, reward, done), where probability is a float number, next_state_index is an integer, reward is a float number, done is a boolean value indicating whether the task is finished.\n",
    "* gamma: discount factor\n",
    "* V: value function, which is a list of float numbers, each float number represents the value for the corresponding state index.\n",
    "\n",
    "The output arguments are:\n",
    "* policy: optimal policy, which is a list of integers, each integer represents the action index for the corresponding state index.\n",
    "\n",
    "```python\n",
    "    def get_max_action_value(self, s: int, env_nA: int, env_transition: Callable, \n",
    "                             V: np.ndarray, gamma: float):\n",
    "```\n",
    "This function is used to get the max action value. The input arguments are:\n",
    "* s: state\n",
    "* env_nA: number of actions\n",
    "* env_transition: transition function, which returns a list of tuples (probability, next_state_index, reward, done), where probability is a float number, next_state_index is an integer, reward is a float number, done is a boolean value indicating whether the task is finished.\n",
    "* V: value function, which is a list of float numbers, each float number represents the value for the corresponding state index.\n",
    "* gamma: discount factor\n",
    "\n",
    "The output arguments are:\n",
    "* max_value: max action value, which is a float number.\n",
    "* max_action: action that leads to max action value, which is an integer.\n",
    "\n",
    "```python\n",
    "    def get_action_value(self, s: int, a: int, V: np.ndarray, gamma: float, \n",
    "                         env_transition: Callable):\n",
    "```\n",
    "This function is used to get the action value. The input arguments are:\n",
    "* s: state\n",
    "* a: action\n",
    "* V: value function, which is a list of float numbers, each float number represents the value for the corresponding state index.\n",
    "* gamma: discount factor\n",
    "* env_transition: transition function, which returns a list of tuples (probability, next_state_index, reward, done), where probability is a float number, next_state_index is an integer, reward is a float number, done is a boolean value indicating whether the task is finished.\n",
    "\n",
    "The output arguments are:\n",
    "* value: action value, which is a float number.\n",
    "\n",
    "You can fill in your implementation in the next following code block. **Remember, copy your implmentation to the `programming.py` file before you submit your assignment. Otherwise, you cannot get any score for coding part.**\n",
    "\n",
    "You can add your own comments at here:\n",
    "\n",
    "--- Your comments starts here ---\n",
    "\n",
    "\n",
    "---- Your comments ends here ----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_action_value(s: int, env_nA: int, env_transition: Callable, \n",
    "                            V: np.ndarray, gamma: float):\n",
    "    \"\"\"\n",
    "    Code for getting max action value. Takes in the current state and returns \n",
    "    the max action value and action that leads to it. I.e., compute\n",
    "    a* = argmax_a \\sum_{s'} p(s'| s, a) * [r + gamma * V(s')]\n",
    "    args:\n",
    "        s: state\n",
    "        env_nA: number of actions\n",
    "        env_transition: transition function\n",
    "        V: value function\n",
    "        gamma: discount factor\n",
    "    returns:\n",
    "        max_value: max action value\n",
    "        max_action: action that leads to max action value\n",
    "    \"\"\"\n",
    "    max_value = -np.inf\n",
    "    max_action = -1\n",
    "    # ------- your code starts here ------- #\n",
    "\n",
    "\n",
    "    # ------- your code ends here ------- #\n",
    "    return max_value, max_action\n",
    "\n",
    "# get action value\n",
    "def get_action_value(s: int, a: int, V: np.ndarray, gamma: float, \n",
    "                        env_transition: Callable):\n",
    "    \"\"\"\n",
    "    Code for getting action value. Compute the value of taking action a in state s\n",
    "    I.e., compute Q(s, a) = \\sum_{s'} p(s'| s, a) * [r + gamma * V(s')]\n",
    "    args:\n",
    "        s: state\n",
    "        a: action\n",
    "        V: value function\n",
    "        gamma: discount factor\n",
    "        env_transition: transition function\n",
    "    returns:\n",
    "        value: action value\n",
    "    \"\"\"\n",
    "    value = 0\n",
    "    # ------- your code starts here ------- #\n",
    "\n",
    "\n",
    "    # ------- your code ends here ------- #\n",
    "    return value\n",
    "\n",
    "# get policy\n",
    "def get_policy(env_nS: int, env_nA: int, env_transition: Callable, \n",
    "                gamma: float, V: np.ndarray):\n",
    "    \"\"\"\n",
    "    Code for getting policy. Takes in an Value function and returns the optimal policy\n",
    "    args:\n",
    "        env_nS: number of states\n",
    "        env_nA: number of actions\n",
    "        env_transition: transition function\n",
    "        gamma: discount factor\n",
    "        V: value function\n",
    "    returns:\n",
    "        policy: policy\n",
    "    \"\"\"\n",
    "    policy = np.zeros(env_nS)\n",
    "    # ------- your code starts here ------- #\n",
    "\n",
    "\n",
    "    # ------- your code ends here ------- #\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Value Iteration\n",
    "```python\n",
    "    def value_iteration(self, gamma: float, theta: float, env_nS: int, env_nA: int, \n",
    "                        env_transition: Callable):\n",
    "```\n",
    "This function is used to implement value iteration. You are expected to implement the value iteration algorithm in this function. The input arguments are:\n",
    "* gamma: discount factor\n",
    "* theta: convergence threshold\n",
    "* env_nS: number of states\n",
    "* env_nA: number of actions\n",
    "* env_transition: transition function, which returns a list of tuples (probability, next_state_index, reward, done), where probability is a float number, next_state_index is an integer, reward is a float number, done is a boolean value indicating whether the task is finished.\n",
    "\n",
    "The output arguments are:\n",
    "* policy: optimal policy, which is a list of integers, each integer represents the action index for the corresponding state index.\n",
    "* V: optimal value function, which is a list of float numbers, each float number represents the value for the corresponding state index.\n",
    "\n",
    "You can fill in your implementation in the next following code block. **Remember, copy your implmentation to the `programming.py` file before you submit your assignment. Otherwise, you cannot get any score for coding part.**\n",
    "\n",
    "You can add your own comments at here:\n",
    "\n",
    "--- Your comments starts here ---\n",
    "\n",
    "\n",
    "---- Your comments ends here ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(gamma: float, theta: float, env_nS: int, env_nA: int, \n",
    "                    env_transition: Callable):\n",
    "    \"\"\"\n",
    "    The code for value iteration. Takes in an MDP and returns the optimal policy\n",
    "    and value function.\n",
    "    args:\n",
    "        gamma: discount factor\n",
    "        theta: convergence threshold\n",
    "        env_nS: number of states\n",
    "        env_nA: number of actions\n",
    "        env_transition: transition function\n",
    "    returns:\n",
    "        policy: optimal policy\n",
    "        V: optimal value function \n",
    "    \"\"\"\n",
    "    V = np.zeros(env_nS)\n",
    "    converged = False\n",
    "    # ------- your code starts here ------- #\n",
    "\n",
    "\n",
    "    # ------- your code ends here ------- #\n",
    "    policy = get_policy(env_nS, env_nA, env_transition, gamma, V)\n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Policy Iterations\n",
    "\n",
    "```python\n",
    "    def policy_evaluation(self, env_nS: int, env_transition: Callable, \n",
    "                          V: np.ndarray, gamma: float, theta: float, policy: np.ndarray):\n",
    "```\n",
    "This function is used to implement policy iteration. You are expected to implement the policy evaluation algorithm in this function. The input arguments are:\n",
    "* env_nS: number of states\n",
    "* env_transition: transition function, which returns a list of tuples (probability, next_state_index, reward, done), where probability is a float number, next_state_index is an integer, reward is a float number, done is a boolean value indicating whether the task is finished.\n",
    "* V: value function, which is a list of float numbers, each float number represents the value for the corresponding state index.\n",
    "* gamma: discount factor\n",
    "* theta: convergence threshold\n",
    "* policy: policy, which is a list of integers, each integer represents the action index for the corresponding state index.\n",
    "\n",
    "The output arguments are:\n",
    "* V: value function, which is a list of float numbers, each float number represents the value for the corresponding state index.\n",
    "\n",
    "```python\n",
    "    def policy_improvement(self, env_nS: int, env_nA: int, env_transition: Callable, \n",
    "                           policy: np.ndarray, V: np.ndarray, gamma: float):\n",
    "```\n",
    "This function is used to implement policy iteration. You are expected to implement the policy improvement algorithm in this function. The input arguments are:\n",
    "* env_nS: number of states\n",
    "* env_nA: number of actions\n",
    "* env_transition: transition function, which returns a list of tuples (probability, next_state_index, reward, done), where probability is a float number, next_state_index is an integer, reward is a float number, done is a boolean value indicating whether the task is finished.\n",
    "* policy: policy, which is a list of integers, each integer represents the action index for the corresponding state index.\n",
    "* gamma: discount factor\n",
    "* V: value function, which is a list of float numbers, each float number represents the value for the corresponding state index.\n",
    "\n",
    "\n",
    "The output arguments are:\n",
    "* policy_stable: whether policy is stable, which is a boolean value.\n",
    "* policy: optimal policy, which is a list of integers, each integer represents the action index for the corresponding state index.\n",
    "\n",
    "\n",
    "You can fill in your implementation in the next following code block. **Remember, copy your implmentation to the `programming.py` file before you submit your assignment. Otherwise, you cannot get any score for coding part.**\n",
    "\n",
    "You can add your own comments at here:\n",
    "\n",
    "--- Your comments starts here ---\n",
    "\n",
    "\n",
    "---- Your comments ends here ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(gamma: float, theta: float, env_nS: int, env_nA: int, \n",
    "                        env_transition: Callable):\n",
    "    \"\"\"\n",
    "    Code for policy iteration. Takes in an MDP and returns the optimal policy\n",
    "    args:\n",
    "        gamma: discount factor\n",
    "        theta: convergence threshold\n",
    "        env_nS: number of states\n",
    "        env_nA: number of actions\n",
    "        env_transition: transition function\n",
    "    returns:\n",
    "        policy: optimal policy\n",
    "        V: optimal value function\n",
    "    \"\"\"\n",
    "    V = np.zeros(env_nS)\n",
    "    policy = np.zeros(env_nS)\n",
    "    converged = False\n",
    "    while not converged:\n",
    "        V = policy_evaluation(env_nS, env_transition, V, gamma, theta, policy)\n",
    "        converged, policy = policy_improvement(\n",
    "            env_nS, env_nA, env_transition, policy, V, gamma)\n",
    "    return policy, V\n",
    "\n",
    "def policy_evaluation(env_nS: int, env_transition: Callable, \n",
    "                        V: np.ndarray, gamma: float, theta: float, policy: np.ndarray):\n",
    "    \"\"\"\n",
    "    Code for policy evaluation. Takes in an MDP and returns the converged value function\n",
    "    args:\n",
    "        env_nS: number of states\n",
    "        env_transition: transition function\n",
    "        V: value function\n",
    "        gamma: discount factor\n",
    "        theta: convergence threshold\n",
    "        policy: policy\n",
    "    returns:\n",
    "        V: value function\n",
    "    \"\"\" \n",
    "    \n",
    "    # ------- your code starts here ------- #\n",
    "\n",
    "\n",
    "    # ------- your code ends here ------- #\n",
    "    return V\n",
    "\n",
    "# policy improvement\n",
    "def policy_improvement(env_nS: int, env_nA: int, env_transition: Callable, \n",
    "                        policy: np.ndarray, V: np.ndarray, gamma: float):\n",
    "    \"\"\"\n",
    "    Code for policy improvement. Takes in an MDP and returns the converged policy\n",
    "    args:\n",
    "        env_nS: number of states\n",
    "        env_nA: number of actions\n",
    "        env_transition: transition function\n",
    "        policy: policy\n",
    "        V: value function\n",
    "        gamma: discount factor\n",
    "    returns:\n",
    "        policy_stable: whether policy is stable\n",
    "        policy: policy\n",
    "    \"\"\"\n",
    "    policy_stable = True\n",
    "    # ------- your code starts here ------- #\n",
    "\n",
    "\n",
    "    # ------- your code ends here ------- #\n",
    "    return policy_stable, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try on your own!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from minihouse.robotminihousemodel import MiniHouseV1\n",
    "from minihouse.minihousev1 import test_cases\n",
    "def test(VI_bool=True, index=0, verbose=False):\n",
    "\n",
    "    instruction, goal_state, initial_conditions = test_cases[index]\n",
    "\n",
    "    env = MiniHouseV1(\n",
    "        instruction=instruction,\n",
    "        goal_state=goal_state,\n",
    "        initial_conditions=initial_conditions,\n",
    "        verbose=verbose\n",
    "    )\n",
    "    if verbose:\n",
    "        print(\"initial state index: \", env.state_to_index(env.state))\n",
    "        print(\"num state: \", env.nS)\n",
    "        print(\"num actions: \", env.nA)\n",
    "\n",
    "\n",
    "    if VI_bool:\n",
    "        policy, V = value_iteration(0.9, 0.0001, env_nS=env.nS, env_nA=env.nA, env_transition=env.transition)\n",
    "        if verbose:\n",
    "            print(\"Value Iteration\")\n",
    "    else:\n",
    "        policy, V = policy_iteration(0.9, 0.0001, env_nS=env.nS, env_nA=env.nA, env_transition=env.transition)\n",
    "        if verbose:\n",
    "            print(\"Policy Iteration\")\n",
    "    if verbose:\n",
    "        print(\"V: \", repr(V))\n",
    "        print(\"Policy: \", repr(policy))\n",
    "\n",
    "    # Checking of solution. Comment out this block if you wish to test other scenarios.    \n",
    "    solution_values = np.loadtxt(f'data/V_{index}.py')\n",
    "    assert len(V) == len(solution_values), \\\n",
    "        'Length of Values is incorrect'\n",
    "    assert np.allclose(V, solution_values), \\\n",
    "        'Values incorrect'\n",
    "    print('\\n\\n--------------------------Your Solution is Correct--------------------------\\n\\n')\n",
    "\n",
    "    obs, _, _, _, _ = env.reset()\n",
    "    print(\"initial obs: \", obs)\n",
    "    for i in range(100):\n",
    "        # print(f\"########## step: {i} #########\")\n",
    "        action = int(policy[env.state_to_index(env.state)])\n",
    "        obs, reward, done, _, _ = env.step(action)\n",
    "        if verbose:\n",
    "            print(\"obs: \", obs)\n",
    "            print(\"reward: \", reward)\n",
    "            print(\"done: \", done, \"\\n\")\n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can try the given test environment to test your implementation. Run the following code to test your implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run Value Iteration, set VI_Bool = True\n",
    "# To run Policy Iteration, set VI_Bool = False\n",
    "test(index=0, VI_bool=True, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional question\n",
    "If the Minihouse has a larger environment consisting of 10 movable items and more rooms, can Value Iteration or Policy Iteration work? What about 100 moveable objects? Can you use pre-trained large language models such as GPT4 to improve the capability of the AI agent in solving large-scale decision-making problems? \n",
    "\n",
    "--- Your comments starts here ---\n",
    "\n",
    "--- Your comments ends here ---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
